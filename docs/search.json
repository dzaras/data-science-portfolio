[
  {
    "objectID": "serenely-case-study.html",
    "href": "serenely-case-study.html",
    "title": "Case Study: Analyzing User Churn for the Serenely App",
    "section": "",
    "text": "import sys\nprint(sys.executable)\n\nc:\\Users\\dima\\Desktop\\tndh_ds_demo\\env\\python.exe\nimport os\nprint(os.getcwd())\n\nc:\\Users\\dima\\Documents\\ds_projects\\my-portfolio"
  },
  {
    "objectID": "serenely-case-study.html#setup-and-data-loading",
    "href": "serenely-case-study.html#setup-and-data-loading",
    "title": "Case Study: Analyzing User Churn for the Serenely App",
    "section": "1. Setup and Data Loading",
    "text": "1. Setup and Data Loading\nThis section imports the necessary Python libraries and loads the three datasets: app usage, user demographics, and user experience surveys. The datasets are then merged into a single DataFrame for analysis.\n\nimport os\nos.environ['OPENBLAS_NUM_THREADS'] = '1'\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nimport lightgbm as lgb\nimport shap\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# Set plotting style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.dpi'] = 100\n\n# --- Load Data ---\ntry:\n    usage_df = pd.read_csv(\"App_usage_data_-_Humu_case_study.csv\")\n    demographics_df = pd.read_csv(\"Demographics_-_Humu_case_study.csv\")\n    survey_df = pd.read_csv(\"User_experience_survey_-_Humu_case_study.csv\")\nexcept FileNotFoundError as e:\n    print(f\"Error loading data: {e}\")\n    print(\"Please ensure the CSV files are in the same directory as this script.\")\n\n# --- Initial Merge & Cleaning ---\n# Reshape survey data from long to wide\nsurvey_wide_df = survey_df.pivot_table(index='userid', columns='time', values=survey_df.columns.drop(['userid', 'time']))\nsurvey_wide_df.columns = ['_'.join(col) for col in survey_wide_df.columns.values]\nsurvey_wide_df.reset_index(inplace=True)\n\n# Merge all three dataframes\ndata = pd.merge(demographics_df, usage_df, on='userid', how='left')\ndata = pd.merge(data, survey_wide_df, on='userid', how='left')\n\nprint(\"Data loaded and merged successfully.\")\nprint(f\"Initial shape of combined data: {data.shape}\")\ndata.head()\n\nData loaded and merged successfully.\nInitial shape of combined data: (3811, 26)\n\n\nc:\\Users\\dima\\Desktop\\tndh_ds_demo\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n\n\n\n\n\n\nuserid\nage_range\ngender\nmetro_area\nday1\nday2\nday3\nday4\nday5\nday6\n...\nday13\nday14\nable_focus_present_sign up\neasy_new_habits_sign up\neasy_to_concet_sign up\nenjoy_reg_sched_sign up\nexpect_app_help_sign up\nfound_app_help_post-14 days\nrecently_stressed_post-14 days\nrecently_stressed_sign up\n\n\n\n\n0\n1\n25 to 34\nfemale\nNYC\n2\n1\n1\n1\n1\n0\n...\n1.0\n1.0\n6.0\n7.0\n6.0\n6.0\n3.0\n5.0\n5.0\n5.0\n\n\n1\n2\n25 to 34\nfemale\nLA\n1\n1\n1\n0\n0\n0\n...\n1.0\n0.0\n6.0\n6.0\n4.0\n4.0\n5.0\n7.0\n3.0\n7.0\n\n\n2\n3\n45 to 54\nfemale\nSF\n1\n1\n4\n0\n0\n0\n...\n1.0\n1.0\n4.0\n5.0\n5.0\n4.0\n2.0\n6.0\n4.0\n7.0\n\n\n3\n4\n25 to 34\nfemale\nNYC\n0\n0\n0\n0\n0\n0\n...\nNaN\nNaN\n4.0\n6.0\n4.0\n7.0\n1.0\n4.0\n5.0\n5.0\n\n\n4\n5\n25 to 34\nfemale\nCHI\n1\n0\n0\n0\n1\n2\n...\n0.0\nNaN\n4.0\n5.0\n5.0\n3.0\n3.0\n7.0\n7.0\n4.0\n\n\n\n\n5 rows × 26 columns"
  },
  {
    "objectID": "serenely-case-study.html#advanced-feature-engineering",
    "href": "serenely-case-study.html#advanced-feature-engineering",
    "title": "Case Study: Analyzing User Churn for the Serenely App",
    "section": "2. Advanced Feature Engineering",
    "text": "2. Advanced Feature Engineering\nHere, new features are created to capture user behavior more effectively. This includes defining churn, calculating usage trends, and measuring changes in user sentiment over the first 14 days.\n\n# This will print a list of all column names in your DataFrame\nprint(\"Columns available in the 'data' DataFrame:\")\nprint(list(data.columns))\n\nColumns available in the 'data' DataFrame:\n['userid', 'age_range', 'gender', 'metro_area', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6', 'day7', 'day8', 'day9', 'day10', 'day11', 'day12', 'day13', 'day14', 'able_focus_present_sign up', 'easy_new_habits_sign up', 'easy_to_concet_sign up', 'enjoy_reg_sched_sign up', 'expect_app_help_sign up', 'found_app_help_post-14 days', 'recently_stressed_post-14 days', 'recently_stressed_sign up']\n\n\n\nprint(\"Starting advanced feature engineering...\")\n\n# --- Churn and Usage Metrics ---\nusage_days = [f'day{i}' for i in range(1, 15)]\ndata['churned'] = data['day14'].isna().astype(int)\n\n# --- Usage Trend Calculation ---\ndef calculate_trend(row):\n    days = np.arange(1, 15)\n    # Use only non-NA values for trend calculation\n    usage = row[usage_days].dropna()\n    if len(usage) &lt; 2:\n        return 0\n    \n    # Simple linear regression to find the slope (trend)\n    model = LinearRegression()\n    model.fit(np.arange(1, len(usage) + 1).reshape(-1, 1), usage.values.reshape(-1, 1))\n    return model.coef_[0][0]\n\nprint(\"Calculating usage trends for each user...\")\ndata['usage_trend'] = data.apply(calculate_trend, axis=1)\n\n# Now fill NaNs with 0 for calculation purposes\ndata[usage_days] = data[usage_days].fillna(0)\n\ndata['total_usage'] = data[usage_days].sum(axis=1)\ndata['days_active'] = (data[usage_days] &gt; 0).sum(axis=1)\ndata['usage_consistency'] = data[usage_days].std(axis=1)\ndata['avg_daily_usage_when_active'] = data['total_usage'] / data['days_active'].replace(0, 1)\n\n\n# --- Sentiment Change Metrics ---\n# Calculate the change in sentiment for questions asked at both sign-up and post-14\n\n# The column 'found_app_help_sign up' does not exist in the data, so we can only calculate stress_change.\nsentiment_cols = {\n    'recently_stressed': ('recently_stressed_sign up', 'recently_stressed_post-14 days'),\n}\n\ndata['stress_change'] = data[sentiment_cols['recently_stressed'][1]] - data[sentiment_cols['recently_stressed'][0]]\n\n# We have to remove the 'helpfulness_change' calculation as the required column is missing.\n# data['helpfulness_change'] = data[sentiment_cols['found_app_help'][1]] - data[sentiment_cols['found_app_help'][0]]\n\n# Rename sign-up survey columns for clarity in modeling\ndata.rename(columns={\n    'expect_app_help_sign up': 'initial_expect_help',\n    'easy_to_concet_sign up': 'initial_easy_to_concentrate',\n    'able_focus_present_sign up': 'initial_focus_present',\n    'easy_new_habits_sign up': 'initial_easy_new_habits',\n    'enjoy_reg_sched_sign up': 'initial_enjoy_schedule',\n    'recently_stressed_sign up': 'initial_stress'\n}, inplace=True)\n\nprint(\"Sentiment features adjusted for available data.\")\n\nStarting advanced feature engineering...\nCalculating usage trends for each user...\nSentiment features adjusted for available data."
  },
  {
    "objectID": "serenely-case-study.html#behavioral-user-segmentation-k-means-clustering",
    "href": "serenely-case-study.html#behavioral-user-segmentation-k-means-clustering",
    "title": "Case Study: Analyzing User Churn for the Serenely App",
    "section": "3. Behavioral User Segmentation (K-Means Clustering)",
    "text": "3. Behavioral User Segmentation (K-Means Clustering)\nK-Means clustering is used to group users into distinct personas based on their behavior. The Elbow Method helps determine the optimal number of clusters.\n\nprint(\"Performing user segmentation with K-Means clustering...\")\n\n# Select features for clustering\ncluster_features = [\n    'total_usage',\n    'days_active',\n    'usage_consistency',\n    'usage_trend',\n    'churned'\n]\n\n# Prepare the data for clustering\ncluster_data = data[cluster_features].copy()\ncluster_data.fillna(cluster_data.mean(), inplace=True) # Fill any remaining NaNs\n\n# Scale features\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(cluster_data)\n\n# --- Elbow Method to find optimal K ---\ninertia = []\nk_range = range(2, 10)\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(scaled_features)\n    inertia.append(kmeans.inertia_)\n\n# Plot the Elbow Method\nplt.figure(figsize=(8, 5))\nplt.plot(k_range, inertia, 'bo-')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia')\nplt.title('Elbow Method for Optimal k')\nplt.show()\n\nPerforming user segmentation with K-Means clustering...\n\n\n\n\n\n\n\n\n\nBased on the elbow plot, k=4 appears to be a good choice. Now, we fit the K-Means model with 4 clusters and profile the resulting personas.\n\n# --- Fit K-Means with chosen K and Profile Clusters ---\nOPTIMAL_K = 4\nkmeans = KMeans(n_clusters=OPTIMAL_K, random_state=42, n_init=10)\ndata['user_persona_cluster'] = kmeans.fit_predict(scaled_features)\n\nprint(f\"\\nProfiling {OPTIMAL_K} User Personas:\")\npersona_profiles = data.groupby('user_persona_cluster')[cluster_features].mean()\nprint(persona_profiles)\n\n# Create descriptive names for the personas based on the profiles\n# We sort by a key metric to assign labels consistently.\nsorted_profiles = persona_profiles.sort_values('total_usage')\npersona_map = {\n    sorted_profiles.index[0]: \"Early Churners\",\n    sorted_profiles.index[1]: \"Fading Casuals\",\n    sorted_profiles.index[2]: \"Consistent Dabblers\",\n    sorted_profiles.index[3]: \"Power Users\",\n}\ndata['user_persona'] = data['user_persona_cluster'].map(persona_map)\n\nprint(\"\\nPersona names assigned:\")\nprint(data[['userid', 'user_persona']].head())\n\n\nProfiling 4 User Personas:\n                      total_usage  days_active  usage_consistency  \\\nuser_persona_cluster                                                \n0                       14.399790     8.713536           1.050092   \n1                        7.425598     4.969823           0.828187   \n2                        0.990373     0.825511           0.207392   \n3                        8.681051     5.508443           0.894453   \n\n                      usage_trend   churned  \nuser_persona_cluster                         \n0                        0.012122  0.026233  \n1                       -0.017354  0.000000  \n2                       -0.005074  0.304452  \n3                       -0.020597  1.000000  \n\nPersona names assigned:\n   userid         user_persona\n0       1          Power Users\n1       2       Fading Casuals\n2       3          Power Users\n3       4       Early Churners\n4       5  Consistent Dabblers"
  },
  {
    "objectID": "serenely-case-study.html#predictive-modeling-churn-engagement",
    "href": "serenely-case-study.html#predictive-modeling-churn-engagement",
    "title": "Case Study: Analyzing User Churn for the Serenely App",
    "section": "4. Predictive Modeling (Churn & Engagement)",
    "text": "4. Predictive Modeling (Churn & Engagement)\nTwo separate LightGBM models are built: 1. A classification model to predict user churn. 2. A regression model to predict the total usage for users who do not churn.\n\nprint(\"Building predictive models...\")\n\n# --- Prepare data for models ---\nmodel_data = data.copy()\n\n# One-hot encode categorical features\ncategorical_cols = ['gender', 'metro_area', 'age_range']\nmodel_data = pd.get_dummies(model_data, columns=categorical_cols, drop_first=True)\n\n# Define feature set (X) and targets (y)\nfeatures_to_drop = [\n    'userid', 'user_persona_cluster', 'user_persona', # Identifiers/leaky features\n    'churned', 'total_usage', # Targets\n    # Drop post-survey data that would leak info about churn/usage\n    'recently_stressed_post-14 days', 'found_app_help_post-14 days',\n    'stress_change', 'helpfulness_change'\n] + usage_days\n\n# Add any other post-survey columns to the drop list\npost_survey_cols = [col for col in model_data.columns if 'post-14 days' in col]\nfeatures_to_drop.extend(post_survey_cols)\n\n# Define feature set X\nX = model_data.drop(columns=features_to_drop, errors='ignore')\n# Ensure all columns are numeric, filling NaNs\nX = X.apply(pd.to_numeric, errors='coerce').fillna(0)\n\n\n# --- Model 1: Churn Prediction (Classification) ---\nprint(\"Training Churn Prediction Model...\")\ny_churn = data['churned']\n\nlgb_churn = lgb.LGBMClassifier(random_state=42)\nlgb_churn.fit(X, y_churn)\n\n# --- Model 2: Engagement Prediction (Regression) ---\n# For users who did NOT churn\nprint(\"Training Engagement Prediction Model...\")\nnon_churners = data[data['churned'] == 0]\nX_engage = X.loc[non_churners.index]\ny_engage = non_churners['total_usage']\n\nlgb_engage = lgb.LGBMRegressor(random_state=42)\nlgb_engage.fit(X_engage, y_engage)\n\nprint(\"Models trained successfully.\")\n\nBuilding predictive models...\nTraining Churn Prediction Model...\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 1344, number of negative: 2467\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000129 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 541\n[LightGBM] [Info] Number of data points in the train set: 3811, number of used features: 27\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.352663 -&gt; initscore=-0.607353\n[LightGBM] [Info] Start training from score -0.607353\nTraining Engagement Prediction Model...\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000382 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 465\n[LightGBM] [Info] Number of data points in the train set: 2467, number of used features: 27\n[LightGBM] [Info] Start training from score 8.513579\nModels trained successfully."
  },
  {
    "objectID": "serenely-case-study.html#model-interpretation-with-shap",
    "href": "serenely-case-study.html#model-interpretation-with-shap",
    "title": "Case Study: Analyzing User Churn for the Serenely App",
    "section": "5. Model Interpretation with SHAP",
    "text": "5. Model Interpretation with SHAP\nSHAP (SHapley Additive exPlanations) is used to interpret the black-box LightGBM models. This helps explain which features are driving the predictions for both churn and engagement, providing clear and actionable insights.\n\n# --- 5. MODEL INTERPRETATION WITH SHAP (Corrected Code) ---\nprint(\"\\nGenerating SHAP plots for model interpretation...\")\n\n# --- SHAP for Churn Model ---\nexplainer_churn = shap.TreeExplainer(lgb_churn)\nshap_values_churn = explainer_churn.shap_values(X)\n\n# --- Create Churn Model Plots ---\n# The bar plot for feature importance usually works with either format\nplt.figure()\nshap.summary_plot(shap_values_churn, X, plot_type=\"bar\", show=False)\nplt.title(\"Feature Importance for Churn Prediction\")\nplt.tight_layout()\nplt.savefig('churn_shap_importance.png')\nprint(\"Churn model feature importance plot saved.\")\n\n# The beeswarm summary plot needs the SHAP values for the \"churn\" class.\n# We add a check to handle different library versions gracefully.\nplt.figure()\n\n# If shap_values is a list (old behavior), it contains values for both classes. We want the second one (for class 1).\nif isinstance(shap_values_churn, list):\n    print(\"Detected list format for SHAP values. Using element 1 for churn class.\")\n    shap.summary_plot(shap_values_churn[1], X, show=False)\n# If it's not a list (new behavior), it's a single array of values for the positive class.\nelse:\n    print(\"Detected numpy array format for SHAP values. Using the array directly.\")\n    shap.summary_plot(shap_values_churn, X, show=False)\n\nplt.title(\"SHAP Summary for Churn Prediction\")\nplt.tight_layout()\nplt.savefig('churn_shap_summary.png')\nprint(\"Churn model SHAP summary plot saved.\")\n\n\n# --- SHAP for Engagement Model (This part should be fine as it's a regression model) ---\nexplainer_engage = shap.TreeExplainer(lgb_engage)\nshap_values_engage = explainer_engage.shap_values(X_engage)\n\nplt.figure()\nshap.summary_plot(shap_values_engage, X_engage, plot_type=\"bar\", show=False)\nplt.title(\"Feature Importance for Engagement (Non-Churned Users)\")\nplt.tight_layout()\nplt.savefig('engagement_shap_importance.png')\nprint(\"Engagement model feature importance plot saved.\")\n\nplt.figure()\nshap.summary_plot(shap_values_engage, X_engage, show=False)\nplt.title(\"SHAP Summary for Engagement Prediction\")\nplt.tight_layout()\nplt.savefig('engagement_shap_summary.png')\nprint(\"Engagement model SHAP plots saved.\")\n\n\nGenerating SHAP plots for model interpretation...\nChurn model feature importance plot saved.\nDetected numpy array format for SHAP values. Using the array directly.\nChurn model SHAP summary plot saved.\nEngagement model feature importance plot saved.\nEngagement model SHAP plots saved.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# --- SHAP for Engagement Model ---\nexplainer_engage = shap.TreeExplainer(lgb_engage)\nshap_values_engage = explainer_engage.shap_values(X_engage)\n\nprint(\"Engagement Model SHAP Plots:\")\nplt.figure()\nshap.summary_plot(shap_values_engage, X_engage, plot_type=\"bar\", show=False)\nplt.title(\"Feature Importance for Engagement (Non-Churned Users)\")\nplt.tight_layout()\nplt.show()\n\nplt.figure()\nshap.summary_plot(shap_values_engage, X_engage, show=False)\nplt.title(\"SHAP Summary for Engagement Prediction\")\nplt.tight_layout()\nplt.show()\n\nEngagement Model SHAP Plots:"
  },
  {
    "objectID": "serenely-case-study.html#final-recommendations",
    "href": "serenely-case-study.html#final-recommendations",
    "title": "Case Study: Analyzing User Churn for the Serenely App",
    "section": "6. Final Recommendations",
    "text": "6. Final Recommendations\nBased on the analysis, here are the key data-driven recommendations:\n1. Focus on the First 7 Days to Combat Churn: - Insight: The usage_trend and days_active features are top predictors of churn. Users whose usage fades in the first week are highly likely to churn. - Recommendation: Implement a “First Week Rescue” program. If a user is inactive for 2-3 consecutive days within their first week, trigger a targeted push notification or email with a gentle nudge, like “Ready for your next 5-minute session?”\n2. Target Personas with Tailored Strategies: - Insight: We identified four distinct personas: Power Users, Consistent Dabblers, Fading Casuals, and Early Churners. - Recommendation for “Fading Casuals”: This group shows declining interest. A/B test different re-engagement strategies for them, such as introducing new content or highlighting the benefits they reported wanting in their sign-up survey. - Recommendation for “Power Users”: Nurture this group. They are your best customers. Offer them advanced content, beta features, or a community to keep them engaged long-term.\n3. Leverage Initial Sentiments for Onboarding: - Insight: Initial survey answers (e.g., initial_expect_help, initial_enjoy_schedule) are strong predictors of both engagement and churn. - Recommendation: Customize the user onboarding flow based on their survey answers. If a user reports high stress and expects the app to help, immediately guide them to a “Stress Relief” program. If they enjoy schedules, prompt them to set a daily meditation reminder. This personalizes the experience from Day 1 and demonstrates value immediately.\n4. Shift Focus from Demographics to Behavior: - Insight: While demographics like age and gender have some predictive power, they are far less important than behavioral features (usage patterns) and initial psychographics (survey answers). - Recommendation: Instead of focusing marketing on “males” (as a previous analysis suggested), focus marketing messages on the benefits that appeal to high-engagement users (e.g., “Find your focus,” “Build a consistent habit”). Target users based on interests and needs, not just who they are."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "image: profile.jpg # —\nHello! I’m Dimitris Zaras, a data scientist with a passion for uncovering insights from complex datasets and building predictive models that drive business decisions. My background in Sociology, Public Health, and Finance gives me a unique perspective on how to apply statistical rigor to solve real-world problems. I am currently seeking opportunities where I can leverage my skills in machine learning and data storytelling to make a tangible impact."
  },
  {
    "objectID": "about.html#technical-skills",
    "href": "about.html#technical-skills",
    "title": "About Me",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nLanguages: Python (Pandas, NumPy, Scikit-learn), SQL, R (Tidyverse)\nMachine Learning: Predictive Modeling (Classification, Regression), Clustering (K-Means), Feature Engineering, Model Evaluation\nData Visualization: Matplotlib, Seaborn, Plotly\nDatabases: PostgreSQL, MySQL, BigQuery * Tools & Platforms: Git, GitHub, VS Code, Docker, Quarto ## My Path to Data\n\nMy journey into data science began during my time as a graduate student, where I realized that the most critical business and scientific decisions were increasingly reliant on data. I was fascinated by the process of transforming raw numbers into a clear narrative that could guide strategy. This led me to generalized, where I built my first predictive model and was immediately hooked on the power of machine learning. I thrive on the challenge of structuring a problem, exploring the data, and building a solution that works."
  },
  {
    "objectID": "about.html#beyond-the-code",
    "href": "about.html#beyond-the-code",
    "title": "About Me",
    "section": "Beyond the Code",
    "text": "Beyond the Code\nWhen I’m not wrangling data, you can find me [Your Hobby, e.g., “hiking local trails”, “experimenting with new recipes”, “playing chess”, “following European football”]."
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "About Me",
    "section": "Let’s Connect",
    "text": "Let’s Connect\nI’m always open to discussing new projects or opportunities in the data space. Feel free to connect with me!\n\nLinkedIn * Email ```"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "my-portfolio",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]